{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-04-19T17:56:17.802903Z",
     "iopub.status.busy": "2022-04-19T17:56:17.80241Z",
     "iopub.status.idle": "2022-04-19T18:03:15.638237Z",
     "shell.execute_reply": "2022-04-19T18:03:15.637363Z",
     "shell.execute_reply.started": "2022-04-19T17:56:17.802809Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install stanford-corenlp\n",
    "# !pip install stanfordnlp\n",
    "# !pip install pytorch-transformers\n",
    "# !pip install transformers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os,sys,time\n",
    "import corenlp\n",
    "import stanfordnlp\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForSequenceClassification,BertTokenizer, BertModel, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "sys.path.append('./scripts/') \n",
    "stanfordnlp.download('en')\n",
    "\n",
    "from event_dataset import EventReader, SentenceReader, Parser\n",
    "import util_sentence_classifier as util_sent_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T18:03:15.640694Z",
     "iopub.status.busy": "2022-04-19T18:03:15.640211Z",
     "iopub.status.idle": "2022-04-19T18:08:01.256129Z",
     "shell.execute_reply": "2022-04-19T18:08:01.255194Z",
     "shell.execute_reply.started": "2022-04-19T18:03:15.640636Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = {}\n",
    "\n",
    "args[\"data_dir\"] = \"./data\"\n",
    "args[\"train_file\"] = \"train.ids\"\n",
    "args[\"dev_file\"] = \"dev.ids\"\n",
    "args[\"test_file\"] = \"test.ids\"\n",
    "args[\"emb_file\"] = \"./data/guten.vectors.txt\"\n",
    "\n",
    "args[\"batch_size\"] = 16\n",
    "args[\"emb_size\"] = 100\n",
    "args[\"hidden_size\"] = 100\n",
    "args[\"dropout\"] = 0.5\n",
    "args[\"num_epochs\"] = 100\n",
    "args[\"learning_rate\"] = 0.001\n",
    "args[\"bidir\"] = True\n",
    "args[\"seed\"] = 0\n",
    "args[\"do_train\"] = True\n",
    "args[\"do_eval\"] = True\n",
    "args[\"model\"] = \"word\"\n",
    "args[\"save_path\"] = None\n",
    "args[\"suffix\"] = None\n",
    "args[\"num_layers\"] = 3\n",
    "args[\"oov_vocab\"] = None\n",
    "\n",
    "random.seed(args[\"seed\"])\n",
    "torch.manual_seed(args[\"seed\"])\n",
    "np.random.seed(args[\"seed\"])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "reader = EventReader()\n",
    "parser = Parser()\n",
    "\n",
    "train_sentences, train_events = reader.read_events(args[\"data_dir\"],args[\"train_file\"])\n",
    "# dev_sentences, dev_events = reader.read_events(args[\"data_dir\"], args[\"dev_file\"])\n",
    "dev_sentences, dev_events = reader.read_events(args[\"data_dir\"], args[\"test_file\"])\n",
    "test_sentences, test_events = reader.read_events(args[\"data_dir\"], args[\"test_file\"])\n",
    "\n",
    "train_parse = parser.parse_sequences(train_sentences)\n",
    "dev_parse = parser.parse_sequences(dev_sentences)\n",
    "test_parse = parser.parse_sequences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T18:08:01.261828Z",
     "iopub.status.busy": "2022-04-19T18:08:01.261499Z",
     "iopub.status.idle": "2022-04-19T18:08:18.751681Z",
     "shell.execute_reply": "2022-04-19T18:08:18.750694Z",
     "shell.execute_reply.started": "2022-04-19T18:08:01.261786Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ = util_sent_cf.get_sentences_events(train_sentences,train_events)\n",
    "dev_ = util_sent_cf.get_sentences_events(dev_sentences,dev_events)\n",
    "test_ = util_sent_cf.get_sentences_events(test_sentences,test_events)\n",
    "\n",
    "train_dataloader = util_sent_cf.tokenize_sentences_make_dataloader(train_,2)\n",
    "dev_dataloader = util_sent_cf.tokenize_sentences_make_dataloader(dev_,2)\n",
    "test_dataloader = util_sent_cf.tokenize_sentences_make_dataloader(test_,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T18:08:18.756028Z",
     "iopub.status.busy": "2022-04-19T18:08:18.755836Z",
     "iopub.status.idle": "2022-04-19T18:24:38.672415Z",
     "shell.execute_reply": "2022-04-19T18:24:38.671493Z",
     "shell.execute_reply.started": "2022-04-19T18:08:18.756003Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bert_preprocess_data(data,tokenizer,model):\n",
    "    sentence_vectors = []\n",
    "    sentence_labels = []\n",
    "    for index in range(len(data)):\n",
    "        inputs = tokenizer(data.sentence[index], return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        sentence_vectors.append(outputs.pooler_output.detach().numpy()[0])\n",
    "        del outputs\n",
    "        del inputs\n",
    "        sentence_labels.append(data.label[index])\n",
    "    df_data = pd.DataFrame({\"sentence\":sentence_vectors,\"label\":sentence_labels})\n",
    "    return df_data\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model2 = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\",num_labels = 2)\n",
    "model.load_state_dict(torch.load(\"./trained_model/model_sent_2_0.8705583756345178\",map_location=torch.device('cpu'))[\"model_state_dict\"])\n",
    "# model.to(\"cpu\")\n",
    "\n",
    "# comment the next two lines to use the wihtout finetuned RoBERTa\n",
    "model2.encoder.load_state_dict(model.roberta.encoder.state_dict())\n",
    "model2.embeddings.load_state_dict(model.roberta.embeddings.state_dict())\n",
    "\n",
    "svm_train_data = bert_preprocess_data(train_,tokenizer,model2)\n",
    "svm_dev_data = bert_preprocess_data(dev_,tokenizer,model2)\n",
    "svm_test_data = bert_preprocess_data(test_,tokenizer,model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T16:13:20.846761Z",
     "iopub.status.busy": "2022-04-10T16:13:20.846421Z",
     "iopub.status.idle": "2022-04-10T16:13:56.021517Z",
     "shell.execute_reply": "2022-04-10T16:13:56.020701Z",
     "shell.execute_reply.started": "2022-04-10T16:13:20.846714Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "tSNE plot\n",
    "'''\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "X_plot = TSNE(n_components=2, learning_rate='auto',init='random').fit_transform(np.array(list(svm_train_data.sentence)))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "y_pred = clf.predict(list(svm_train_data.sentence))\n",
    "# error = []\n",
    "# for i in range(len(svm_train_data)):\n",
    "#         if(y_pred[i] != svm_train_data.label[i]):\n",
    "#             error.append(1)\n",
    "#         else:\n",
    "#             error.append(0)\n",
    "sns.scatterplot(X_plot[:,0],X_plot[:,1],hue=np.array(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T05:37:04.609154Z",
     "iopub.status.busy": "2022-04-18T05:37:04.608294Z",
     "iopub.status.idle": "2022-04-18T05:37:14.931128Z",
     "shell.execute_reply": "2022-04-18T05:37:14.93023Z",
     "shell.execute_reply.started": "2022-04-18T05:37:04.609074Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "SVM classifier\n",
    "'''\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "clf_svm = make_pipeline(StandardScaler(), SVC(kernel=\"rbf\",C=2, gamma=0.001))\n",
    "clf_svm.fit(list(svm_train_data.sentence), np.array(svm_train_data.label))\n",
    "\n",
    "y_pred = clf_svm.predict(list(svm_dev_data.sentence))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(np.array(svm_dev_data.label), y_pred)*100)\n",
    "print(\"Precision:\",metrics.precision_score(np.array(svm_dev_data.label), y_pred)*100)\n",
    "print(\"Recall:\",metrics.recall_score(np.array(svm_dev_data.label), y_pred)*100)\n",
    "\n",
    "print(\"No. of support vectors per data point : \",len(clf_svm[1].support_vectors_)/len(train_))\n",
    "print(\"No. of support vectors : \",len(clf_svm[1].support_vectors_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T19:45:09.062013Z",
     "iopub.status.busy": "2022-04-10T19:45:09.061718Z",
     "iopub.status.idle": "2022-04-10T19:45:09.076296Z",
     "shell.execute_reply": "2022-04-10T19:45:09.075215Z",
     "shell.execute_reply.started": "2022-04-10T19:45:09.061968Z"
    }
   },
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(dev_.sentence[wrong_index],copy=True)\n",
    "dataframe = dataframe.reset_index().drop(columns=\"index\")\n",
    "dataframe.to_csv(\"./out_all_1.csv\")\n",
    "# dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T19:41:20.474359Z",
     "iopub.status.busy": "2022-04-10T19:41:20.474098Z",
     "iopub.status.idle": "2022-04-10T19:41:20.520495Z",
     "shell.execute_reply": "2022-04-10T19:41:20.519436Z",
     "shell.execute_reply.started": "2022-04-10T19:41:20.474333Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in dataframe.sentence:\n",
    "    print(i,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T05:41:01.611442Z",
     "iopub.status.busy": "2022-04-18T05:41:01.611186Z",
     "iopub.status.idle": "2022-04-18T05:41:13.477153Z",
     "shell.execute_reply": "2022-04-18T05:41:13.475904Z",
     "shell.execute_reply.started": "2022-04-18T05:41:01.611412Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Random Forest classifier\n",
    "'''\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "clf_rf = make_pipeline(StandardScaler(), RandomForestClassifier(max_depth=15, random_state=1))\n",
    "clf_rf.fit(list(svm_train_data.sentence), np.array(svm_train_data.label))\n",
    "\n",
    "y_pred = clf_rf.predict(list(svm_dev_data.sentence))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(np.array(svm_dev_data.label), y_pred)*100)\n",
    "print(\"Precision:\",metrics.precision_score(np.array(svm_dev_data.label), y_pred)*100)\n",
    "print(\"Recall:\",metrics.recall_score(np.array(svm_dev_data.label), y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T05:40:53.499965Z",
     "iopub.status.busy": "2022-04-18T05:40:53.49955Z",
     "iopub.status.idle": "2022-04-18T05:41:01.60958Z",
     "shell.execute_reply": "2022-04-18T05:41:01.608593Z",
     "shell.execute_reply.started": "2022-04-18T05:40:53.499923Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "XGBoost classifier\n",
    "'''\n",
    "import xgboost as xgb\n",
    "\n",
    "clf_xg = make_pipeline(StandardScaler(), xgb.XGBClassifier(objective='binary:logistic', n_estimators=16))\n",
    "clf_xg.fit(list(svm_train_data.sentence), np.array(svm_train_data.label))\n",
    "\n",
    "y_pred = clf_xg.predict(list(svm_dev_data.sentence))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(np.array(svm_dev_data.label), y_pred)*100)\n",
    "print(\"Precision:\",metrics.precision_score(np.array(svm_dev_data.label), y_pred)*100)\n",
    "print(\"Recall:\",metrics.recall_score(np.array(svm_dev_data.label), y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T18:56:21.691928Z",
     "iopub.status.busy": "2022-04-10T18:56:21.691592Z",
     "iopub.status.idle": "2022-04-10T18:56:21.698767Z",
     "shell.execute_reply": "2022-04-10T18:56:21.697918Z",
     "shell.execute_reply.started": "2022-04-10T18:56:21.691882Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Grid Search\n",
    "'''\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# parameters = {'kernel':['rbf'], 'C':[2*x for x in range(1,15)],'gamma':[0.01*x for x in range(1,90)]}\n",
    "# clf_grid = GridSearchCV(SVC(), parameters,verbose=3)\n",
    "# clf_grid.fit(list(svm_train_data.sentence), np.array(svm_train_data.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T04:57:30.060309Z",
     "iopub.status.busy": "2022-04-18T04:57:30.058844Z",
     "iopub.status.idle": "2022-04-18T04:57:30.833902Z",
     "shell.execute_reply": "2022-04-18T04:57:30.832876Z",
     "shell.execute_reply.started": "2022-04-18T04:57:30.060248Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "KNN classifier\n",
    "'''\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=8)\n",
    "knn.fit(list(svm_train_data.sentence), np.array(svm_train_data.label))\n",
    "\n",
    "y_pred = knn.predict(list(svm_dev_data.sentence))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(np.array(svm_dev_data.label), y_pred)*100)\n",
    "print(\"Precision:\",metrics.precision_score(np.array(svm_dev_data.label), y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(np.array(svm_dev_data.label), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T18:56:24.16394Z",
     "iopub.status.busy": "2022-04-10T18:56:24.163099Z",
     "iopub.status.idle": "2022-04-10T18:56:24.913678Z",
     "shell.execute_reply": "2022-04-10T18:56:24.912707Z",
     "shell.execute_reply.started": "2022-04-10T18:56:24.163886Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "linear classifier over Roberta embeddings\n",
    "'''\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf2 = make_pipeline(StandardScaler(),SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "clf2.fit(list(svm_train_data.sentence), np.array(svm_train_data.label))\n",
    "\n",
    "y_pred = clf2.predict(list(svm_dev_data.sentence))\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(np.array(svm_dev_data.label), y_pred)*100)\n",
    "print(\"Precision:\",metrics.precision_score(np.array(svm_dev_data.label), y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(np.array(svm_dev_data.label), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-10T19:45:03.111818Z",
     "iopub.status.busy": "2022-04-10T19:45:03.111456Z",
     "iopub.status.idle": "2022-04-10T19:45:04.876808Z",
     "shell.execute_reply": "2022-04-10T19:45:04.87596Z",
     "shell.execute_reply.started": "2022-04-10T19:45:03.111786Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Analysis to get intersection\n",
    "'''\n",
    "\n",
    "y_pred = {}\n",
    "y_pred[\"svm\"] = clf_svm.predict(list(svm_dev_data.sentence))\n",
    "y_pred[\"rf\"] = clf_rf.predict(list(svm_dev_data.sentence))\n",
    "y_pred[\"xg\"] = clf_xg.predict(list(svm_dev_data.sentence))\n",
    "y_pred[\"rf\"] = knn.predict(list(svm_dev_data.sentence))\n",
    "y_pred[\"linear\"] = clf2.predict(list(svm_dev_data.sentence))\n",
    "\n",
    "wrong_index = []\n",
    "# for j in range(12):\n",
    "#     count = 0\n",
    "#     actual_count = 0\n",
    "for i in range(len(svm_dev_data)):\n",
    "#         if(dev_.no_of_events[i]==j):\n",
    "#             actual_count += 1\n",
    "    true_count = np.sum([y_pred[key][i] != svm_dev_data.label[i] for key in y_pred])\n",
    "    if(true_count==len(y_pred.keys()) and dev_.no_of_events[i]>0):\n",
    "#         print(dev_.sentence[i],dev_.no_of_events[i],\"\\n\")\n",
    "        wrong_index.append(i)\n",
    "#             count += 1\n",
    "#     print(j,\" : \",count,\"...\",actual_count,\" fraction = \",(count*100/actual_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-04-03T04:04:41.904762Z",
     "iopub.status.busy": "2022-04-03T04:04:41.904324Z",
     "iopub.status.idle": "2022-04-03T06:07:34.337991Z",
     "shell.execute_reply": "2022-04-03T06:07:34.33645Z",
     "shell.execute_reply.started": "2022-04-03T04:04:41.904722Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Roberta-sentence classifier (freeze / non-freeze transformer layers)\n",
    "'''\n",
    "freeze_transformer_layers = False\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\",num_labels = 2)\n",
    "\n",
    "if(freeze_transformer_layers):\n",
    "    for param in model.roberta.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "best_acc = 0\n",
    "model.to(device)\n",
    "for epoch in range(30):\n",
    "    epoch_loss=0\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        batch_loss=loss.item()\n",
    "        optim.step()\n",
    "        epoch_loss+=batch_loss\n",
    "    acc = util_sent_cf.test(model,dev_dataloader,device)\n",
    "    normalized_epoch_loss = epoch_loss/(len(train_dataloader))\n",
    "    if(best_acc<acc):\n",
    "        best_acc = acc\n",
    "#         torch.save(model,\"model_sent_\"+str(best_acc))\n",
    "        torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'loss': normalized_epoch_loss,\n",
    "            'dev_accuracy': acc\n",
    "            }, \"model_sent_\"+str(epoch+1)+\"_\"+str(acc))\n",
    "    \n",
    "    print(\"Epoch {}\".format(epoch+1))\n",
    "    print(\"Epoch loss: {} \".format(normalized_epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T14:51:50.953655Z",
     "iopub.status.busy": "2022-03-21T14:51:50.953311Z",
     "iopub.status.idle": "2022-03-21T16:20:11.242995Z",
     "shell.execute_reply": "2022-03-21T16:20:11.240552Z",
     "shell.execute_reply.started": "2022-03-21T14:51:50.953615Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "N-gram data generation\n",
    "'''\n",
    "from numpy import linalg as LA\n",
    "\n",
    "def get_dimensions(train_data,dev_data):\n",
    "    bigram_to_index = {}\n",
    "    index_to_bigram = []\n",
    "\n",
    "    for index,row in train_data.iterrows():\n",
    "        #label = int(row[\"label\"])\n",
    "        try:\n",
    "            if row[\"sentence\"][-1] == '.':\n",
    "                row[\"sentence\"] = row[\"sentence\"][:-1]\n",
    "        except:\n",
    "            print(index)\n",
    "            print(row[\"sentence\"])\n",
    "            continue\n",
    "        unigrams = row[\"sentence\"].split(\" \")[:-1]\n",
    "        bigrams = [b for b in zip(row[\"sentence\"].split(\" \")[:-1], row[\"sentence\"].split(\" \")[1:])]\n",
    "        bigrams += unigrams\n",
    "        for b in bigrams:\n",
    "            key = \" \".join(b)\n",
    "            if key not in bigram_to_index.keys():\n",
    "                bigram_to_index[key] = len(index_to_bigram)\n",
    "                index_to_bigram.append(key)\n",
    "        if (index+1)%1000 == 0:\n",
    "            print(\"{} examples finshed\".format(index+1))\n",
    "\n",
    "    for index,row in dev_data.iterrows():\n",
    "        #label = int(row[\"label\"])\n",
    "        if row[\"sentence\"][-1] == '.':\n",
    "            row[\"sentence\"] = row[\"sentence\"][:-1]\n",
    "        unigrams = row[\"sentence\"].split(\" \")[:-1]\n",
    "        bigrams = [b for b in zip(row[\"sentence\"].split(\" \")[:-1], row[\"sentence\"].split(\" \")[1:])]\n",
    "        bigrams += unigrams\n",
    "        for b in bigrams:\n",
    "            key = \" \".join(b)\n",
    "            if key not in bigram_to_index.keys():\n",
    "                bigram_to_index[key] = len(index_to_bigram)\n",
    "                index_to_bigram.append(key)\n",
    "        if (index+1)%1000 == 0:\n",
    "            print(\"{} examples finished\".format(index+1))\n",
    "\n",
    "    return bigram_to_index, index_to_bigram\n",
    "\n",
    "\n",
    "def get_data(data, bigram_to_index):\n",
    "    trainable_data = np.zeros((1,len(bigram_to_index)))\n",
    "    labels = np.array([])\n",
    "    for index,row in data.iterrows():\n",
    "        labels = np.append(labels,int(row[\"label\"]))\n",
    "        data_point = np.zeros((1,len(bigram_to_index)))\n",
    "        if row[\"sentence\"][-1] == '.':\n",
    "            row[\"sentence\"] = row[\"sentence\"][:-1]\n",
    "        unigrams = row[\"sentence\"].split(\" \")[:-1]\n",
    "        bigrams = [b for b in zip(row[\"sentence\"].split(\" \")[:-1], row[\"sentence\"].split(\" \")[1:])]\n",
    "        bigrams += unigrams\n",
    "        for b in bigrams:\n",
    "            key = \" \".join(b)\n",
    "            data_point[0][bigram_to_index[key]] = 1\n",
    "        trainable_data = np.append(trainable_data,data_point,axis=0)\n",
    "\n",
    "        if (index+1)%1000 == 0:\n",
    "            print(\"{} examples finished\".format(index))\n",
    "\n",
    "    trainable_data = trainable_data[1:]\n",
    "    return trainable_data, labels\n",
    "\n",
    "bigram_to_index, index_to_bigram = get_dimensions(train_,dev_)\n",
    "train_data_final = get_data(train_,bigram_to_index)\n",
    "dev_data_final = get_data(dev_,bigram_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T16:20:11.250397Z",
     "iopub.status.busy": "2022-03-21T16:20:11.249898Z",
     "iopub.status.idle": "2022-03-21T16:35:41.875911Z",
     "shell.execute_reply": "2022-03-21T16:35:41.875062Z",
     "shell.execute_reply.started": "2022-03-21T16:20:11.250343Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "SVM classifier for n-gram\n",
    "'''\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), SVC(kernel=\"linear\",gamma='auto',C=24))\n",
    "clf.fit(train_data_final[0], train_data_final[1])\n",
    "\n",
    "y_pred = clf.predict(dev_data_final[0])\n",
    "print(\"Accuracy:\",metrics.accuracy_score(dev_data_final[1], y_pred)*100)\n",
    "print(\"Precision:\",metrics.precision_score(dev_data_final[1], y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(dev_data_final[1], y_pred))\n",
    "\n",
    "print(\"No. of support vectors per data point : \",len(clf[1].support_vectors_)/len(train_))\n",
    "print(\"No. of support vectors : \",len(clf[1].support_vectors_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
