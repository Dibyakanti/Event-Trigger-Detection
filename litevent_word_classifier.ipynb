{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-04-19T17:49:05.670333Z",
     "iopub.status.busy": "2022-04-19T17:49:05.670061Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "!pip install stanford-corenlp\n",
    "!pip install stanfordnlp\n",
    "!pip install pytorch-transformers\n",
    "!pip install transformers\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os,sys,time\n",
    "import corenlp\n",
    "import stanfordnlp\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer, BertModel, BertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW\n",
    "\n",
    "sys.path.append('./scripts/')\n",
    "stanfordnlp.download('en')\n",
    "\n",
    "from event_dataset import EventReader, SentenceReader, Parser\n",
    "from word_models import EventExtractor\n",
    "from da_models_new import AdversarialEventExtractor, GradReverse\n",
    "from bert_embedding_extractor import BertFeatureExtractor\n",
    "import util_sentence_classifier as util_sent_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "\n",
    "args[\"data_dir\"] = \"./data\"\n",
    "args[\"train_file\"] = \"train.ids\"\n",
    "args[\"dev_file\"] = \"dev.ids\"\n",
    "args[\"test_file\"] = \"test.ids\"\n",
    "args[\"emb_file\"] = \"./data/guten.vectors.txt\"\n",
    "\n",
    "args[\"batch_size\"] = 16\n",
    "args[\"emb_size\"] = 100\n",
    "args[\"hidden_size\"] = 100\n",
    "args[\"dropout\"] = 0.5\n",
    "args[\"num_epochs\"] = 100\n",
    "args[\"learning_rate\"] = 0.001\n",
    "args[\"bidir\"] = True\n",
    "args[\"seed\"] = 0\n",
    "args[\"do_train\"] = True\n",
    "args[\"do_eval\"] = True\n",
    "args[\"model\"] = \"word\"\n",
    "args[\"save_path\"] = None\n",
    "args[\"suffix\"] = None\n",
    "args[\"num_layers\"] = 3\n",
    "args[\"oov_vocab\"] = None\n",
    "\n",
    "random.seed(args[\"seed\"])\n",
    "torch.manual_seed(args[\"seed\"])\n",
    "np.random.seed(args[\"seed\"])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "reader = EventReader()\n",
    "parser = Parser()\n",
    "\n",
    "train_sentences, train_events = reader.read_events(args[\"data_dir\"],args[\"train_file\"])\n",
    "# dev_sentences, dev_events = reader.read_events(args[\"data_dir\"], args[\"dev_file\"])\n",
    "dev_sentences, dev_events = reader.read_events(args[\"data_dir\"], args[\"test_file\"])\n",
    "test_sentences, test_events = reader.read_events(args[\"data_dir\"], args[\"test_file\"])\n",
    "\n",
    "train_parse = parser.parse_sequences(train_sentences)\n",
    "dev_parse = parser.parse_sequences(dev_sentences)\n",
    "test_parse = parser.parse_sequences(test_sentences)\n",
    "\n",
    "train_ = util_sent_cf.get_sentences_events(train_sentences,train_events)\n",
    "dev_ = util_sent_cf.get_sentences_events(dev_sentences,dev_events)\n",
    "test_ = util_sent_cf.get_sentences_events(test_sentences,test_events)\n",
    "\n",
    "train_dataloader = util_sent_cf.tokenize_sentences_make_dataloader(train_,2)\n",
    "dev_dataloader = util_sent_cf.tokenize_sentences_make_dataloader(dev_,2)\n",
    "test_dataloader = util_sent_cf.tokenize_sentences_make_dataloader(test_,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_event_classifier(model,sentence_in):\n",
    "\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    sentences = tokenizer(sentence_in,padding = True,truncation=True,max_length = 512,return_tensors='pt')\n",
    "    output = model(sentences.input_ids,attention_mask=sentences.attention_mask)\n",
    "    logit = output[0].detach().cpu().numpy()\n",
    "    pred = np.argmax(logit,axis=1).flatten()\n",
    "\n",
    "    return pred\n",
    "\n",
    "model_sent = RobertaForSequenceClassification.from_pretrained(\"roberta-base\",num_labels = 2)\n",
    "checkpoint = torch.load(\"./trained_model/model_sent_3_0.8705583756345178\")\n",
    "model_sent.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sentence_prediction, model_sent, dev_sentences, model, train_batches, dev_batches, num_epochs, learning_rate, use_cuda, path, model_type):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(list(model.rep_learner.parameters()) + list(model.classifier.parameters()), lr=learning_rate)\n",
    "    best_precision, best_recall, best_f1 = 0.0, 0.0, 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        num_batches = len(train_batches)\n",
    "        for batch in train_batches:\n",
    "            optimizer.zero_grad()\n",
    "            batch = [x.to('cuda') for x in batch]\n",
    "            labels, predictions = batch[-3], []\n",
    "            labels = labels.contiguous().view(-1,1)\n",
    "            if model_type == \"word\" or model_type == \"delex\" or model_type.startswith(\"bert\"):\n",
    "                sents, _, lengths, masks = batch\n",
    "                predictions = model(sents, lengths.cpu(), masks)\n",
    "            elif model_type == \"pos\":\n",
    "                sents, pos, _, lengths, masks = batch\n",
    "                predictions = model(sents, lengths.cpu(), masks, pos)\n",
    "            loss = criterion(predictions, labels)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_loss /= num_batches\n",
    "        print(\"Training Loss at epoch {}: {}\".format(epoch, total_loss))\n",
    "        print(\"Performance on development set:\")\n",
    "        precision, recall, f1 = test(sentence_prediction, model_sent, dev_sentences, model, dev_batches, use_cuda, '', args[\"model\"])\n",
    "        if f1 > best_f1:\n",
    "            best_precision = precision\n",
    "            best_recall = recall\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), path)\n",
    "        model.train()\n",
    "\n",
    "def test(sentence_prediction, model_sent, dev_sentences, model, dev_batches, use_cuda, path, model_type, oov=None):\n",
    "    if path != '':\n",
    "        model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    index = 0\n",
    "    predicted, gold, correct = 0.0, 0.0, 0.0\n",
    "    # all_reps = []\n",
    "    iv_predicted, iv_gold, iv_correct = 0.0, 0.0, 0.0\n",
    "    oov_predicted, oov_gold, oov_correct = 0.0, 0.0, 0.0\n",
    "    for batch in dev_batches:\n",
    "        cpu_sents = batch[0].view(-1,1)\n",
    "        batch = [x.to('cuda') for x in batch]\n",
    "        labels, predictions = batch[-3], []\n",
    "        labels = labels.contiguous().view(-1,1)\n",
    "        \n",
    "        # Integrating my code here\n",
    "        sent_predictions = []\n",
    "        for j in range(len(batch[2])):\n",
    "            length = batch[2][j]\n",
    "            for i in range(length):\n",
    "                sent_predictions.append(sentence_prediction[index+j])\n",
    "                \n",
    "        if model_type == \"word\" or model_type == \"delex\" or model_type.startswith(\"bert\"):\n",
    "            sents, _, lengths, masks = batch\n",
    "            predictions = model(sents, lengths.cpu(), masks)   # Remove reps after dumping BERT\n",
    "        elif model_type == \"pos\":\n",
    "            sents, pos, _, lengths, masks = batch\n",
    "            predictions = model(sents, lengths.cpu(), masks, pos)\n",
    "        if use_cuda:\n",
    "            predictions = predictions.cpu().detach().numpy()\n",
    "            labels = labels.cpu().detach().numpy()\n",
    "            # reps = reps.cpu()  # Comment out after dumping BERT\n",
    "        cur_correct, cur_pred, cur_gold = calculate_batch_f1(predictions.tolist(), labels.tolist(), sent_predictions)\n",
    "        if oov is not None:\n",
    "            cur_iv_correct, cur_iv_pred, cur_iv_gold, cur_oov_correct, cur_oov_pred, cur_oov_gold = calculate_split_f1(predictions.tolist(), labels.tolist(), oov, cpu_sents, sent_vocab)\n",
    "            iv_correct += cur_iv_correct\n",
    "            iv_gold += cur_iv_gold\n",
    "            iv_predicted += cur_iv_pred\n",
    "            oov_correct += cur_oov_correct\n",
    "            oov_gold += cur_oov_gold\n",
    "            oov_predicted += cur_oov_pred\n",
    "        predicted += cur_pred\n",
    "        gold += cur_gold\n",
    "        correct += cur_correct\n",
    "        index += len(batch[0])\n",
    "        # all_reps.append(reps)\n",
    "    # pickle.dump(all_reps, open('BERT_reps_rec.pkl', 'wb'))\n",
    "    # print('Dumped BERT record reps')\n",
    "    precision = correct / predicted if predicted != 0 else 0.0\n",
    "    recall = correct / gold if gold != 0 else 0.0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if precision + recall != 0 else 0.0\n",
    "    print(\"Precision: {}\".format(precision))\n",
    "    print(\"Recall: {}\".format(recall))\n",
    "    print(\"F1 Score: {}\".format(f1))\n",
    "    return precision, recall, f1\n",
    "\n",
    "def calculate_split_f1(preds, labels, oov, raw_sents, sent_vocab):\n",
    "    iv_predicted, iv_gold, iv_correct = 0.0, 0.0, 0.0\n",
    "    oov_predicted, oov_gold, oov_correct = 0.0, 0.0, 0.0\n",
    "    raw_sents = raw_sents.tolist()\n",
    "    # print(len(raw_sents))\n",
    "    # exit(1)\n",
    "    i = 0\n",
    "    for pred, label in zip(preds, labels):\n",
    "        pred = 0 if pred[0] <= 0.0 else 1\n",
    "        label = label[0]\n",
    "        cur_words = raw_sents[i]\n",
    "        #if pred == 1:\n",
    "        i += 1\n",
    "\n",
    "def calculate_batch_f1(preds, labels, sent_preds):\n",
    "    predicted = 0.0\n",
    "    gold = 0.0\n",
    "    correct = 0.0\n",
    "    for pred, label, sent_pred in zip(preds, labels, sent_preds):\n",
    "        pred = (0*sent_pred) if pred[0] <= 0.0 else (1*sent_pred)\n",
    "        label = label[0]\n",
    "        if pred == 1:\n",
    "            predicted += 1\n",
    "        if label == 1:\n",
    "            gold += 1\n",
    "        if pred == label and label == 1:\n",
    "            correct += 1\n",
    "    return correct, predicted, gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ONLY take the sentences which have atleast 1 label\n",
    "'''\n",
    "\n",
    "def filter_sentences(sentences,events,dataframe):\n",
    "    filtered_sentences = []\n",
    "    filtered_events = []\n",
    "    for i in list(dataframe[dataframe.label==1].index):\n",
    "        filtered_sentences.append(sentences[i])\n",
    "        filtered_events.append(events[i])\n",
    "        \n",
    "    return filtered_sentences,filtered_events\n",
    "\n",
    "f_train_sentences,f_train_events = filter_sentences(train_sentences,train_events,train_)\n",
    "f_dev_sentences,f_dev_events = filter_sentences(dev_sentences,dev_events,dev_)\n",
    "f_test_sentences,f_test_events = filter_sentences(dev_sentences,dev_events,dev_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vocab = reader.construct_vocab(train_sentences + dev_sentences + test_sentences)\n",
    "pos_vocab = reader.construct_vocab(train_parse + dev_parse + test_parse)\n",
    "label_vocab = {\"O\": 0, \"EVENT\": 1, \"ENT\": 0}\n",
    "\n",
    "use_shared_vocab = True\n",
    "use_filtered_sentences_only = False\n",
    "\n",
    "args[\"model_path\"] = \"./\"\n",
    "oov_vocab = None\n",
    "if args[\"oov_vocab\"] is not None:\n",
    "    pickle.load(open(args[\"oov_vocab\"], 'rb'))\n",
    "\n",
    "if args[\"do_train\"]:\n",
    "    pickle.dump(pos_vocab, open(args[\"model_path\"]+\"_posvocab_{}.pkl\".format(args[\"seed\"]), \"wb\"))\n",
    "    if use_shared_vocab:\n",
    "        sent_vocab = pickle.load(open(\"./scripts/shared_vocab_news_lit.pkl\", \"rb\"))\n",
    "    pickle.dump(sent_vocab, open(args[\"model_path\"]+\"_vocab_{}.pkl\".format(args[\"seed\"]), \"wb\"))\n",
    "elif args[\"do_eval\"]:\n",
    "    pos_vocab = pickle.load(open(args[\"model_path\"]+\"_posvocab_{}.pkl\".format(args[\"seed\"]), \"rb\"))\n",
    "    sent_vocab = pickle.load(open(args[\"model_path\"]+\"_vocab_{}.pkl\".format(args[\"seed\"]), \"rb\"))\n",
    "\n",
    "if(use_filtered_sentences_only):\n",
    "    int_train_sents = reader.construct_integer_sequences(f_train_sentences, sent_vocab)\n",
    "    int_train_labels = reader.construct_integer_sequences(f_train_events, label_vocab)\n",
    "    int_dev_sents = reader.construct_integer_sequences(dev_sentences, sent_vocab)\n",
    "    int_dev_labels = reader.construct_integer_sequences(dev_events, label_vocab)\n",
    "    int_test_sents = reader.construct_integer_sequences(test_sentences, sent_vocab) # made changes !!!!!!!!!!!! LOOK AT IT\n",
    "    int_test_labels = reader.construct_integer_sequences(test_events, label_vocab)\n",
    "else:\n",
    "    int_train_sents = reader.construct_integer_sequences(train_sentences, sent_vocab)\n",
    "    int_train_labels = reader.construct_integer_sequences(train_events, label_vocab)\n",
    "    int_dev_sents = reader.construct_integer_sequences(dev_sentences, sent_vocab)\n",
    "    int_dev_labels = reader.construct_integer_sequences(dev_events, label_vocab)\n",
    "    int_test_sents = reader.construct_integer_sequences(test_sentences, sent_vocab)\n",
    "    int_test_labels = reader.construct_integer_sequences(test_events, label_vocab)\n",
    "\n",
    "train_batches, dev_batches, test_batches = [], [], []\n",
    "\n",
    "if args[\"model\"] == \"word\":\n",
    "    train_batches = reader.create_padded_batches(int_train_sents, int_train_labels, args[\"batch_size\"], use_cuda, True)\n",
    "    dev_batches = reader.create_padded_batches(int_dev_sents, int_dev_labels, args[\"batch_size\"], use_cuda, False)\n",
    "    test_batches = reader.create_padded_batches(int_test_sents, int_test_labels, args[\"batch_size\"], use_cuda, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if args[\"model\"] == 'word':\n",
    "    model = EventExtractor(len(list(sent_vocab.keys())), args[\"emb_size\"], args[\"hidden_size\"], 1, args[\"dropout\"], args[\"bidir\"], args[\"model\"])\n",
    "\n",
    "if args[\"emb_file\"] is not None:\n",
    "    model.rep_learner.load_embeddings(args[\"emb_file\"], sent_vocab)\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "if args[\"do_train\"]:\n",
    "    sentence_prediction = []\n",
    "    for index in range(0,len(dev_sentences),16):\n",
    "        for s in sentence_event_classifier(model_sent, [\" \".join(dev_sentences[x]) for x in range(index,min(index+16,len(dev_sentences)))] ):\n",
    "            sentence_prediction.append(1)\n",
    "    train(sentence_prediction, model_sent, dev_sentences, model, train_batches, dev_batches, args[\"num_epochs\"], args[\"learning_rate\"], use_cuda, args[\"model_path\"]+\"_{}.pth\".format(args[\"seed\"]), args[\"model\"])\n",
    "    if args[\"do_eval\"]:\n",
    "        test(sentence_prediction, model_sent, dev_sentences, model, test_batches, use_cuda, args[\"model_path\"]+\"_{}.pth\".format(args[\"seed\"]), args[\"model\"], oov=oov_vocab)\n",
    "else:\n",
    "    sentence_prediction = []\n",
    "    for index in range(0,len(dev_sentences),16):\n",
    "        for s in sentence_event_classifier(model_sent, [\" \".join(dev_sentences[x]) for x in range(index,min(index+16,len(dev_sentences)))] ):\n",
    "            sentence_prediction.append(1)\n",
    "    test(sentence_prediction, model_sent, dev_sentences, model, test_batches, use_cuda, args.model_path+\"_{}.pth\".format(args.seed), args.model, oov=oov_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
